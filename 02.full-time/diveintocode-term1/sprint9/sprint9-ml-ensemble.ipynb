{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint9課題 アンサンブル学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小さなデータセットの用意\n",
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。\n",
    "\n",
    "train.csvを学習用（train）8割、検証用（val）2割に分割してください。\n",
    "\n",
    "### scikit-learn\n",
    "単一のモデルはスクラッチ実装ではなく、scikit-learnなどのライブラリの使用を推奨します。\n",
    "\n",
    "[sklearn.linear_model.LinearRegression — scikit-learn 0.20.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "[sklearn.svm.SVR — scikit-learn 0.20.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "[sklearn.tree.DecisionTreeRegressor — scikit-learn 0.20.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールをインポート\n",
    "import numpy as np      #numpy\n",
    "import pandas as pd      #pandas\n",
    "import matplotlib.pyplot as plt      #グラフ\n",
    "import seaborn as sns      #seaborn\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split      #学習データ分割\n",
    "from sklearn.linear_model import LinearRegression      #線形回帰\n",
    "from sklearn.metrics import mean_squared_error      #平均二乗誤差\n",
    "from sklearn.preprocessing import StandardScaler      #標準化ライブラリ\n",
    "from sklearn.svm import SVR     #SVM(回帰)\n",
    "from sklearn.tree import DecisionTreeRegressor   #決定木\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.csvの読み込み\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GrLivAreaとYearBuiltを抜き出す\n",
    "X = df.loc[:, ['GrLivArea', 'YearBuilt']].values\n",
    "\n",
    "#目的変数SalePriceを抜き出す\n",
    "y = df.loc[:, ['SalePrice']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取り込みデータを学習用、検証用に分割する\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】ブレンディング\n",
    "ブレンディングを実装し、単一モデルより精度があがる例を最低3つ示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを示します。\n",
    "\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "- 手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "- ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "- 入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。必ずしも単一モデルの精度が高い必要はありません。\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "**補足**\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n",
    "[sklearn.ensemble.VotingClassifier — scikit-learn 0.20.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)\n",
    "\n",
    "**考察**\n",
    "\n",
    "どういった組み合わせが良いか、どのようにすると多様なモデルが作れるかを考えてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.データの前処理なし・パラメータデフォルトにてブレンディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,955.529\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(X_train, y_train)\n",
    "y_val_pred_lr1 = lr1.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr1))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 88,667.101\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr1 = SVR(gamma='auto')\n",
    "svr1.fit(X_train, y_train.reshape(-1))\n",
    "y_val_pred_svr1 = svr1.predict(X_validation)\n",
    "y_val_pred_svr1 = y_val_pred_svr1.reshape(y_val_pred_svr1.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr1))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 47,516.068\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測\n",
    "dtr1 = DecisionTreeRegressor()\n",
    "dtr1.fit(X_train, y_train)\n",
    "y_val_pred_dtr1 = dtr1.predict(X_validation)\n",
    "y_val_pred_dtr1 = y_val_pred_dtr1.reshape(y_val_pred_dtr1.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr1))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの前処理なし・パラメータデフォルトにてブレンディング\n",
    "y_val_pred_blend1 = np.mean([\n",
    "    y_val_pred_lr1, \n",
    "    y_val_pred_svr1, \n",
    "    y_val_pred_dtr1\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 52,131.358\n"
     ]
    }
   ],
   "source": [
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_blend1))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度は高まらず(線形回帰、決定木より精度が低い)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.データの前処理なし・SVMのみパラメータをチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,955.529\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train, y_train)\n",
    "y_val_pred_lr2 = lr2.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr2))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 51,798.697\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr2 = SVR(gamma='auto', kernel='linear', C=1e3, epsilon=46)\n",
    "svr2.fit(X_train, y_train.reshape(-1))\n",
    "y_val_pred_svr2 = svr2.predict(X_validation)\n",
    "y_val_pred_svr2 = y_val_pred_svr2.reshape(y_val_pred_svr2.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr2))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 48,598.143\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測\n",
    "dtr2 = DecisionTreeRegressor()\n",
    "dtr2.fit(X_train, y_train)\n",
    "y_val_pred_dtr2 = dtr2.predict(X_validation)\n",
    "y_val_pred_dtr2 = y_val_pred_dtr2.reshape(y_val_pred_dtr2.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr2))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 45,721.635\n"
     ]
    }
   ],
   "source": [
    "#ブレンディング\n",
    "y_val_pred_blend2 = np.mean([\n",
    "    y_val_pred_lr2, \n",
    "    y_val_pred_svr2, \n",
    "    y_val_pred_dtr2\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_blend2))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度が高まった**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.データを標準化する(条件は上記2を利用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float型に変換\n",
    "X_train = X_train.astype(np.float)\n",
    "X_validation = X_validation.astype(np.float)\n",
    "y_train = y_train.astype(np.float)\n",
    "y_validation = y_validation.astype(np.float)\n",
    "\n",
    "\n",
    "#学習データにて標準化ライブラリのインスタンス生成\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(X_train)\n",
    "\n",
    "#学習データ、検証データをそれぞれ標準化\n",
    "X_train_st = scaler_X.transform(X_train)\n",
    "X_validation_st = scaler_X.transform(X_validation)\n",
    "\n",
    "#学習データにて標準化ライブラリのインスタンス生成\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(y_train)\n",
    "\n",
    "#学習データ、検証データをそれぞれ標準化\n",
    "y_train_st = scaler_y.transform(y_train)\n",
    "y_validation_st = scaler_y.transform(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,955.529\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr3 = LinearRegression()\n",
    "lr3.fit(X_train_st, y_train)\n",
    "y_val_pred_lr3 = lr3.predict(X_validation_st)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr3))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 52,679.924\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr3 = SVR(gamma='auto', kernel='linear', C=1e3, epsilon=46)\n",
    "svr3.fit(X_train_st, y_train.reshape(-1))\n",
    "y_val_pred_svr3 = svr3.predict(X_validation_st)\n",
    "y_val_pred_svr3 = y_val_pred_svr3.reshape(y_val_pred_svr3.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr3))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 47,929.345\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測\n",
    "dtr3 = DecisionTreeRegressor()\n",
    "dtr3.fit(X_train_st, y_train)\n",
    "y_val_pred_dtr3 = dtr3.predict(X_validation_st)\n",
    "y_val_pred_dtr3 = y_val_pred_dtr3.reshape(y_val_pred_dtr3.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr3))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 45,734.026\n"
     ]
    }
   ],
   "source": [
    "#ブレンディング\n",
    "y_val_pred_blend3 = np.mean([\n",
    "    y_val_pred_lr3, \n",
    "    y_val_pred_svr3, \n",
    "    y_val_pred_dtr3\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_blend3))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度が高まった**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.データを対数変換する(条件は上記2を利用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量データを対数変換する\n",
    "X_train_log = np.log(X_train)\n",
    "X_validation_log = np.log(X_validation)\n",
    "\n",
    "#特徴量データを対数変換する\n",
    "y_train_log = np.log(y_train)\n",
    "y_validation_log = np.log(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 54,530.346\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr4 = LinearRegression()\n",
    "lr4.fit(X_train_log, y_train)\n",
    "y_val_pred_lr4 = lr4.predict(X_validation_log)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr4))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 68,829.644\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr4 = SVR(gamma='auto', kernel='linear', C=1e3, epsilon=46)\n",
    "svr4.fit(X_train_log, y_train.reshape(-1))\n",
    "y_val_pred_svr4 = svr4.predict(X_validation_log)\n",
    "y_val_pred_svr4 = y_val_pred_svr4.reshape(y_val_pred_svr4.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr4))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 46,613.650\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測\n",
    "dtr4 = DecisionTreeRegressor()\n",
    "dtr4.fit(X_train_log, y_train)\n",
    "y_val_pred_dtr4 = dtr4.predict(X_validation_log)\n",
    "y_val_pred_dtr4 = y_val_pred_dtr4.reshape(y_val_pred_dtr4.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr4))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,410.915\n"
     ]
    }
   ],
   "source": [
    "#ブレンディング\n",
    "y_val_pred_blend4 = np.mean([\n",
    "    y_val_pred_lr4, \n",
    "    y_val_pred_svr4, \n",
    "    y_val_pred_dtr4\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_blend4))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度は高まらず(決定木より精度が低い)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.SVMのカーネルの種類を混ぜ合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 51,827.790\n"
     ]
    }
   ],
   "source": [
    "### SVMにて検証用データの予測(カーネル:linear)\n",
    "svr5_linear = SVR(gamma=1, kernel='linear', C=1e5, epsilon=50)\n",
    "svr5_linear.fit(X_train_st, y_train.reshape(-1))\n",
    "y_val_pred_svr5_linear = svr5_linear.predict(X_validation_st)\n",
    "y_val_pred_svr5_linear = y_val_pred_svr5_linear.reshape(y_val_pred_svr5_linear.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr5_linear))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 67,184.879\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測(カーネル:rbf)\n",
    "svr5_rbf = SVR(gamma=1, kernel='rbf', C=1e3)\n",
    "svr5_rbf.fit(X_train_st, y_train.reshape(-1))\n",
    "y_val_pred_svr5_rbf = svr5_rbf.predict(X_validation_st)\n",
    "y_val_pred_svr5_rbf = y_val_pred_svr5_rbf.reshape(y_val_pred_svr5_rbf.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr5_rbf))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 53,433.656\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測(カーネル:sigmoid)\n",
    "svr5_sig = SVR(gamma=0.05, kernel='sigmoid', C=1e5)\n",
    "svr5_sig.fit(X_train_st, y_train.reshape(-1))\n",
    "y_val_pred_svr5_sig = svr5_sig.predict(X_validation_st)\n",
    "y_val_pred_svr5_sig = y_val_pred_svr5_sig.reshape(y_val_pred_svr5_sig.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr5_sig))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 56,259.156\n"
     ]
    }
   ],
   "source": [
    "#ブレンディング\n",
    "y_val_pred_blend5 = np.mean([\n",
    "    y_val_pred_svr5_linear, \n",
    "    y_val_pred_svr5_rbf, \n",
    "    y_val_pred_svr5_sig\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_blend5))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度は高まらず(sigmoidより精度が低い)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.色々チューニングして一番精度を高める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,955.529\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr6 = LinearRegression(fit_intercept = True, normalize = True)\n",
    "lr6.fit(X_train, y_train)\n",
    "y_val_pred_lr6 = lr6.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr6))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 47,945.397\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr6 = SVR(gamma=0.01, kernel='rbf', C=1e6)\n",
    "svr6.fit(X_train_st, y_train.reshape(-1))\n",
    "y_val_pred_svr6 = svr6.predict(X_validation_st)\n",
    "y_val_pred_svr6 = y_val_pred_svr6.reshape(y_val_pred_svr6.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr6))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 42,945.369\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測\n",
    "dtr6 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=5)\n",
    "dtr6.fit(X_train, y_train)\n",
    "y_val_pred_dtr6 = dtr6.predict(X_validation)\n",
    "y_val_pred_dtr6 = y_val_pred_dtr6.reshape(y_val_pred_dtr6.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr6))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 42,440.326\n"
     ]
    }
   ],
   "source": [
    "#ブレンディング\n",
    "y_val_pred_blend6 = np.average([\n",
    "    y_val_pred_lr6, \n",
    "    y_val_pred_svr6, \n",
    "    y_val_pred_dtr6\n",
    "], axis=0, weights=[1,1,8])\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_blend6))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度が高まった**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】バギング\n",
    "バギングを実装し、単一モデルより精度があがる例を最低1つ示してください。\n",
    "\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ブートストラップサンプル）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "[sklearn.model_selection.train_test_split — scikit-learn 0.20.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ブートストラップサンプルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train01, _, y_train01, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.2,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train02, _, y_train02, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.2,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train03, _, y_train03, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.2,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train04, _, y_train04, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.線形回帰モデルにてバギング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,955.529\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測(比較対象)\n",
    "lr_or = LinearRegression(fit_intercept = True, normalize = True)\n",
    "lr_or.fit(X_train, y_train)\n",
    "y_val_pred_lr_or = lr_or.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr_or))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 50,347.794\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr_bg01 = LinearRegression(fit_intercept = True, normalize = True)\n",
    "lr_bg01.fit(X_train01, y_train01)\n",
    "y_val_pred_lr_bg01 = lr_bg01.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr_bg01))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,583.841\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr_bg02 = LinearRegression(fit_intercept = True, normalize = True)\n",
    "lr_bg02.fit(X_train02, y_train02)\n",
    "y_val_pred_lr_bg02 = lr_bg02.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr_bg02))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,979.229\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr_bg03 = LinearRegression(fit_intercept = True, normalize = True)\n",
    "lr_bg03.fit(X_train03, y_train03)\n",
    "y_val_pred_lr_bg03 = lr_bg03.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr_bg03))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,464.220\n"
     ]
    }
   ],
   "source": [
    "#線形回帰にて検証用データの予測\n",
    "lr_bg04 = LinearRegression(fit_intercept = True, normalize = True)\n",
    "lr_bg04.fit(X_train04, y_train04)\n",
    "y_val_pred_lr_bg04 = lr_bg04.predict(X_validation)\n",
    "\n",
    "# 検証用データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr_bg04))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 49,788.100\n"
     ]
    }
   ],
   "source": [
    "#バギング\n",
    "y_val_pred_lr_bg = np.average([\n",
    "    y_val_pred_lr_bg01, \n",
    "    y_val_pred_lr_bg02, \n",
    "    y_val_pred_lr_bg03, \n",
    "    y_val_pred_lr_bg04\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_lr_bg))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度上がらず**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.SVMにてバギング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 51,794.306\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測（比較用）\n",
    "svr_or = SVR(gamma=0.01, kernel='linear', C=1e3)\n",
    "svr_or.fit(X_train, y_train.reshape(-1))\n",
    "y_val_pred_svr_or = svr_or.predict(X_validation)\n",
    "y_val_pred_svr_or = y_val_pred_svr_or.reshape(y_val_pred_svr_or.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr_or))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 52,036.682\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr_bg01 = SVR(gamma=0.01, kernel='linear', C=1e3)\n",
    "svr_bg01.fit(X_train01, y_train01.reshape(-1))\n",
    "y_val_pred_svr_bg01 = svr_bg01.predict(X_validation)\n",
    "y_val_pred_svr_bg01 = y_val_pred_svr_bg01.reshape(y_val_pred_svr_bg01.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr_bg01))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 51,689.700\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr_bg02 = SVR(gamma=0.01, kernel='linear', C=1e3)\n",
    "svr_bg02.fit(X_train02, y_train02.reshape(-1))\n",
    "y_val_pred_svr_bg02 = svr_bg02.predict(X_validation)\n",
    "y_val_pred_svr_bg02 = y_val_pred_svr_bg02.reshape(y_val_pred_svr_bg02.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr_bg02))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 51,872.374\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr_bg03 = SVR(gamma=0.01, kernel='linear', C=1e3)\n",
    "svr_bg03.fit(X_train03, y_train03.reshape(-1))\n",
    "y_val_pred_svr_bg03 = svr_bg03.predict(X_validation)\n",
    "y_val_pred_svr_bg03 = y_val_pred_svr_bg03.reshape(y_val_pred_svr_bg03.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr_bg03))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 52,121.186\n"
     ]
    }
   ],
   "source": [
    "#SVMにて検証用データの予測\n",
    "svr_bg04 = SVR(gamma=0.01, kernel='linear', C=1e3)\n",
    "svr_bg04.fit(X_train04, y_train04.reshape(-1))\n",
    "y_val_pred_svr_bg04 = svr_bg04.predict(X_validation)\n",
    "y_val_pred_svr_bg04 = y_val_pred_svr_bg04.reshape(y_val_pred_svr_bg04.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr_bg04))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 51,922.062\n"
     ]
    }
   ],
   "source": [
    "#バギング\n",
    "y_val_pred_svr_bg = np.average([\n",
    "    y_val_pred_svr_bg01, \n",
    "    y_val_pred_svr_bg02, \n",
    "    y_val_pred_svr_bg03, \n",
    "    y_val_pred_svr_bg04\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_svr_bg))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：元データと比較すると精度は向上した**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.決定木でバギング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 42,945.369\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測（比較検証用）\n",
    "dtr_or = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=5)\n",
    "dtr_or.fit(X_train, y_train)\n",
    "y_val_pred_dtr_or = dtr_or.predict(X_validation)\n",
    "y_val_pred_dtr_or = y_val_pred_dtr_or.reshape(y_val_pred_dtr_or.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr_or))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 56,000.226\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測（比較検証用）\n",
    "dtr_bg01 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=5)\n",
    "dtr_bg01.fit(X_train01, y_train01)\n",
    "y_val_pred_dtr_bg01 = dtr_bg01.predict(X_validation)\n",
    "y_val_pred_dtr_bg01 = y_val_pred_dtr_bg01.reshape(y_val_pred_dtr_bg01.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr_bg01))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 44,152.706\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測（比較検証用）\n",
    "dtr_bg02 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=5)\n",
    "dtr_bg02.fit(X_train02, y_train02)\n",
    "y_val_pred_dtr_bg02 = dtr_bg02.predict(X_validation)\n",
    "y_val_pred_dtr_bg02 = y_val_pred_dtr_bg02.reshape(y_val_pred_dtr_bg02.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr_bg02))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 45,935.982\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測（比較検証用）\n",
    "dtr_bg03 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=5)\n",
    "dtr_bg03.fit(X_train03, y_train03)\n",
    "y_val_pred_dtr_bg03 = dtr_bg03.predict(X_validation)\n",
    "y_val_pred_dtr_bg03 = y_val_pred_dtr_bg03.reshape(y_val_pred_dtr_bg03.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr_bg03))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 42,102.528\n"
     ]
    }
   ],
   "source": [
    "#決定木にて検証用データの予測（比較検証用）\n",
    "dtr_bg04 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=5)\n",
    "dtr_bg04.fit(X_train04, y_train04)\n",
    "y_val_pred_dtr_bg04 = dtr_bg04.predict(X_validation)\n",
    "y_val_pred_dtr_bg04 = y_val_pred_dtr_bg04.reshape(y_val_pred_dtr_bg04.shape[0],1)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr_bg04))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 43,075.844\n"
     ]
    }
   ],
   "source": [
    "#バギング\n",
    "y_val_pred_dtr_bg = np.average([\n",
    "    y_val_pred_dtr_bg01, \n",
    "    y_val_pred_dtr_bg02, \n",
    "    y_val_pred_dtr_bg03, \n",
    "    y_val_pred_dtr_bg04\n",
    "], axis=0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.3f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation, y_val_pred_dtr_bg))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**結果：精度向上した**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】スタッキング\n",
    "**ここから説明変数・目的変数両者に対して対数変換を行う**  \n",
    "\n",
    "スタッキングを実装し、単一モデルより精度があがる例を最低1つ示してください。\n",
    "\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは$K_0=3,M_0=2$程度にします。\n",
    "\n",
    "**学習時**\n",
    "\n",
    "（ステージ $0$ ）\n",
    "\n",
    "- 学習データを$K_0$個に分割する。\n",
    "- 分割した内の $(K_0 - 1)$ 個をまとめて学習用データ、残り $1$ 個を推定用データとする組み合わせが $K_0$ 個作れる。\n",
    "- あるモデルのインスタンスを $K_0$ 個用意し、異なる学習用データを使い学習する。\n",
    "- それぞれの学習済みモデルに対して、使っていない残り $1$ 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "- さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$個あれば、 $M_0$ 個のブレンドデータが得られる。\n",
    "\n",
    "\n",
    "（ステージ $n$ ）\n",
    "\n",
    "- ステージ $n-1$ のブレンドデータを$M_{n-1}$次元の特徴量を持つ学習用データと考え、 $K_n$個に分割する。以下同様である。\n",
    "\n",
    "\n",
    "（ステージ $N$ ）＊最後のステージ\n",
    "\n",
    "- ステージ $N-1$ の $M_{N-1}$ 個のブレンドデータを$M_{N-1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "\n",
    "**推定時**\n",
    "\n",
    "（ステージ $0$ ）\n",
    "\n",
    "- テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$個の推定値を得る。これを $K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "\n",
    "（ステージ $n$ ）\n",
    "\n",
    "- ステージ $n-1$ で得たブレンドテストを $K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。これを $K_n$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "\n",
    "（ステージ $N$ ）＊最後のステージ\n",
    "\n",
    "- ステージ$N-1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動きを確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習ステージ0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#インデックスを三分割\n",
    "X_idx = np.random.permutation(np.arange(0, X_train_log.shape[0], 1))\n",
    "f0_idx, f1_idx, f2_idx = np.array_split(X_idx, 3)\n",
    "f1_f2_idx = np.concatenate([f1_idx, f2_idx])\n",
    "f0_f2_idx = np.concatenate([f0_idx, f2_idx])\n",
    "f0_f1_idx = np.concatenate([f0_idx, f1_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f0_idxに対するブレンドデータ0を作成\n",
    "s0_reg00 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=3)\n",
    "s0_reg00.fit(X_train_log[f1_f2_idx], y_train_log[f1_f2_idx])\n",
    "brend_data_reg00 = s0_reg00.predict(X_train_log[f0_idx])\n",
    "brend_data_reg00 = brend_data_reg00.reshape(brend_data_reg00.shape[0],1)\n",
    "\n",
    "#f1_idxに対するブレンドデータ0を作成\n",
    "s0_reg01 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=3)\n",
    "s0_reg01.fit(X_train_log[f0_f2_idx], y_train_log[f0_f2_idx])\n",
    "brend_data_reg01 = s0_reg01.predict(X_train_log[f1_idx])\n",
    "brend_data_reg01 = brend_data_reg01.reshape(brend_data_reg01.shape[0],1)\n",
    "\n",
    "#f2_idxに対するブレンドデータを作成\n",
    "s0_reg02 = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=3)\n",
    "s0_reg02.fit(X_train_log[f0_f1_idx], y_train_log[f0_f1_idx])\n",
    "brend_data_reg02 = s0_reg02.predict(X_train_log[f2_idx])\n",
    "brend_data_reg02 = brend_data_reg02.reshape(brend_data_reg02.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f0_idxに対するブレンドデータを作成\n",
    "s0_reg10 = LinearRegression()\n",
    "s0_reg10.fit(X_train_log[f1_f2_idx], y_train_log[f1_f2_idx])\n",
    "brend_data_reg10 = s0_reg10.predict(X_train_log[f0_idx])\n",
    "brend_data_reg10 = brend_data_reg10.reshape(brend_data_reg10.shape[0],1)\n",
    "\n",
    "\n",
    "#f1_idxに対するブレンドデータを作成\n",
    "s0_reg11 = LinearRegression()\n",
    "s0_reg11.fit(X_train_log[f0_f2_idx], y_train_log[f0_f2_idx])\n",
    "brend_data_reg11 = s0_reg11.predict(X_train_log[f1_idx])\n",
    "brend_data_reg11 = brend_data_reg11.reshape(brend_data_reg11.shape[0],1)\n",
    "\n",
    "\n",
    "#f2_idxに対するブレンドデータを作成\n",
    "s0_reg12 = LinearRegression()\n",
    "s0_reg12.fit(X_train_log[f0_f1_idx], y_train_log[f0_f1_idx])\n",
    "brend_data_reg12 = s0_reg12.predict(X_train_log[f2_idx])\n",
    "brend_data_reg12 = brend_data_reg12.reshape(brend_data_reg12.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    " #K0個のbrend_data_reg0に格納\n",
    "brend_data_reg0 = np.zeros( X_train_log.shape[0]).reshape(-1, 1)\n",
    "brend_data_reg0[f0_idx] = brend_data_reg00\n",
    "brend_data_reg0[f1_idx] = brend_data_reg01\n",
    "brend_data_reg0[f2_idx] = brend_data_reg02\n",
    "\n",
    "#K0個brend_data_reg1を結合\n",
    "brend_data_reg1 = np.zeros( X_train_log.shape[0]).reshape(-1, 1)\n",
    "brend_data_reg1[f0_idx] = brend_data_reg10\n",
    "brend_data_reg1[f1_idx] = brend_data_reg11\n",
    "brend_data_reg1[f2_idx] = brend_data_reg12\n",
    "\n",
    "#上記を結合\n",
    "brend_data_s0 = np.concatenate([brend_data_reg0, brend_data_reg1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習ステージ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#インデックスを三分割\n",
    "X_idx = np.random.permutation(np.arange(0, X_train_log.shape[0], 1))\n",
    "f0_idx, f1_idx, f2_idx = np.array_split(X_idx, 3)\n",
    "f1_f2_idx = np.concatenate([f1_idx, f2_idx])\n",
    "f0_f2_idx = np.concatenate([f0_idx, f2_idx])\n",
    "f0_f1_idx = np.concatenate([f0_idx, f1_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ステージ0のデータで再度学習\n",
    "#f0_idxに対するブレンドデータを作成\n",
    "s1_reg00 = LinearRegression()\n",
    "s1_reg00.fit(brend_data_s0[f1_f2_idx], y_train_log[f1_f2_idx])\n",
    "brend_data_s1_reg00 = s1_reg00.predict(brend_data_s0[f0_idx])\n",
    "\n",
    "#f1_idxに対するブレンドデータを作成\n",
    "s1_reg01 = LinearRegression()\n",
    "s1_reg01.fit(brend_data_s0[f0_f2_idx], y_train_log[f0_f2_idx])\n",
    "brend_data_s1_reg01 = s1_reg01.predict(brend_data_s0[f1_idx])\n",
    "\n",
    "#f2_idxに対するブレンドデータを作成\n",
    "s1_reg02 = LinearRegression()\n",
    "s1_reg02.fit(brend_data_s0[f0_f1_idx], y_train_log[f0_f1_idx])\n",
    "brend_data_s1_reg02 = s1_reg02.predict(brend_data_s0[f2_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    " #K1個のbrend_dataを結合\n",
    "brend_data_s1_reg0 = np.zeros( X_train_log.shape[0]).reshape(-1, 1)\n",
    "brend_data_s1_reg0[f0_idx] = brend_data_s1_reg00\n",
    "brend_data_s1_reg0[f1_idx] = brend_data_s1_reg01\n",
    "brend_data_s1_reg0[f2_idx] = brend_data_s1_reg02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習ステージ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#推定用のインスタンス・モデルの生成\n",
    "s2_reg = LinearRegression()\n",
    "s2_reg.fit(brend_data_s1_reg0, y_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推定ステージ0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習ステージ0にて学習したモデルs0_reg0*にてpredict\n",
    "brend_test_reg00 = s0_reg00.predict(X_validation_log)\n",
    "brend_test_reg01 = s0_reg01.predict(X_validation_log)\n",
    "brend_test_reg02 = s0_reg02.predict(X_validation_log)\n",
    "\n",
    "#平均を取得する\n",
    "brend_test_reg0 = np.mean([\n",
    "    brend_test_reg00, \n",
    "    brend_test_reg01, \n",
    "    brend_test_reg02], axis=0).reshape(-1,1)\n",
    "\n",
    "#学習ステージ0にて学習したモデルs0_reg1*にてpredict\n",
    "brend_test_reg10 = s0_reg10.predict(X_validation_log)\n",
    "brend_test_reg11 = s0_reg11.predict(X_validation_log)\n",
    "brend_test_reg12 = s0_reg12.predict(X_validation_log)\n",
    "\n",
    "#平均を取得する\n",
    "brend_test_reg1 = np.mean([\n",
    "    brend_test_reg10, \n",
    "    brend_test_reg11, \n",
    "    brend_test_reg12], axis=0).reshape(-1,1)\n",
    "\n",
    "#上記を結合する\n",
    "brend_test_s0 = np.concatenate([brend_test_reg0, brend_test_reg1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推定ステージ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習ステージ1にて学習したモデルs1_reg0*にてpredict\n",
    "brend_test_s1_reg00 = s1_reg00.predict(brend_test_s0)\n",
    "brend_test_s1_reg01 = s1_reg01.predict(brend_test_s0)\n",
    "brend_test_s1_reg02 = s1_reg02.predict(brend_test_s0)\n",
    "\n",
    "#平均を取得する\n",
    "brend_test_s1_reg0 = np.mean([\n",
    "    brend_test_s1_reg00, \n",
    "    brend_test_s1_reg01, \n",
    "    brend_test_s1_reg02], axis=0).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推定ステージ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 0.2275\n"
     ]
    }
   ],
   "source": [
    "#推定ステージ1にて取得した\n",
    "brend_test_s2 = s2_reg.predict(brend_test_s1_reg0)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.4f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation_log, brend_test_s2))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 線形回帰での平均二乗誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 0.2282\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_log, y_train_log)\n",
    "y_pred_lr = lr.predict(X_validation_log)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.4f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation_log, y_pred_lr))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 決定木での平均二乗誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 0.2563\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor(criterion=\"mse\", random_state=5, max_depth=3)\n",
    "dtr.fit(X_train_log, y_train_log)\n",
    "y_pred_dtr = dtr.predict(X_validation_log)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.4f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation_log, y_pred_dtr))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "スタッキングにより精度が向上した"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスを作成する（写経）\n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightgbmのインスタンス生成\n",
    "model_lgb = lgb.LGBMRegressor(\n",
    "    objective='regression', \n",
    "    num_leabes=5, \n",
    "    learning_rate=0.05, \n",
    "    n_estimators=720, \n",
    "    max_bin=55,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=5, \n",
    "    feature_fraction=0.2319, \n",
    "    feature_fraction_seed=9,\n",
    "    bagging_seed=9, \n",
    "    min_data_in_leaf=6, \n",
    "    min_sum_hessian_in_leaf=11\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#決定木のインスタンス生成\n",
    "model_dtr = DecisionTreeRegressor(\n",
    "    criterion=\"mse\", \n",
    "    random_state=5, \n",
    "    max_depth=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#線形回帰のインスタンス生成\n",
    "model_lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル同士の平均にて学習・予測を行う(≒blending)\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    #データを収めるためのモデルを複製する\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        #複製したモデルにて学習を行う\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    #複製したモデルを用いて予測し、平均する\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 0.2167\n"
     ]
    }
   ],
   "source": [
    "#線形回帰、決定木、lgbmの平均モデルのインスタンス生成\n",
    "averaged_models = AveragingModels(models = (model_lr, model_dtr, model_lgb))\n",
    "\n",
    "#学習データにてモデル作成\n",
    "averaged_models.fit(X_train_log, y_train_log.reshape(-1))\n",
    "\n",
    "#検証用データにて予測\n",
    "ave_pred = averaged_models.predict(X_validation_log)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.4f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation_log, ave_pred))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スタッキングのクラス作成（ステージ0とステージ1の構造）\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    \n",
    "    def  __init__(self, base_models, meta_model, n_folds=3):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "    \n",
    "    #学習\n",
    "    def fit(self, X, y):\n",
    "        #ベースモデル(ステージ0)のモデルより複製を行う\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        \n",
    "        #メタモデルの複製\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        \n",
    "        #データ分割\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        #新たな特徴量とするout_of_fold_predictionsを作成する\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        #それぞれのベースモデルにて学習、推定を行い、out_of_fold_predictionsに格納\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        #新しい特徴量out-of-fold-predictionsを用いて複製したメタモデルを学習する\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    #推定\n",
    "    def predict(self, X):\n",
    "        #ベースモデルより新しい特徴量を作成した上で、メタモデルで予測を行う\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_\n",
    "        ])\n",
    "\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 0.2192\n"
     ]
    }
   ],
   "source": [
    "#スタッキングモデルにてインスタンス生成\n",
    "stacked_averaged_models = StackingAveragedModels(\n",
    "    base_models = (model_dtr, model_lgb), \n",
    "    meta_model = model_lr\n",
    ")\n",
    "\n",
    "#学習を行う\n",
    "stacked_averaged_models.fit(X_train_log, y_train_log.reshape(-1))\n",
    "\n",
    "#予測する\n",
    "stacked_pred = stacked_averaged_models.predict(X_validation_log)\n",
    "\n",
    "#検証データに関して平均二乗誤差を出力\n",
    "print('[RMSE] 検証用データ : {:,.4f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation_log, stacked_pred))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE] 検証用データ : 0.2254\n"
     ]
    }
   ],
   "source": [
    "model_lgb.fit(X_train_log, y_train_log.reshape(-1))\n",
    "\n",
    "lgb_pred = model_lgb.predict(X_validation_log)\n",
    "\n",
    "print('[RMSE] 検証用データ : {:,.4f}'.format(\n",
    "    np.sqrt(mean_squared_error(y_validation_log, lgb_pred))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "LightGBMを加えたスタッキングでも精度が向上した"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】（アドバンス課題）Microsoft Malware Prediction\n",
    "時間的に余裕がある場合は、KaggleのMalwareコンペを題材に、アンサンブル学習を実際に利用してみましょう。EDAからはじめ、推定値の提出まで挑戦してください。\n",
    "\n",
    "[Microsoft Malware Prediction | Kaggle](https://www.kaggle.com/c/microsoft-malware-prediction)\n",
    "です。\n",
    "\n",
    "注意点\n",
    "\n",
    "非常に大きなデータセットなため、扱いに工夫が必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※省略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
