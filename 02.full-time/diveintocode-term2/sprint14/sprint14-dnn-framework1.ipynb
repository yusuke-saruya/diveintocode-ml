{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint14課題 ディープラーニングフレームワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "（例）\n",
    "\n",
    "- 重みを初期化する必要があった\n",
    "- エポックのループが必要だった\n",
    "\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- レイヤー別に種類を指定し、インスタンスを生成する必要があった\n",
    "- 各層に対して次元(shape)を合わせる必要があった\n",
    "- 学習率を引数にて指定する必要があった\n",
    "- 出力サイズを計算する必要があった\n",
    "- 活性化関数(ReLU, sigmoid, tanh)を指定する必要があった\n",
    "- 最適化手法(SGD, AdaGrad)を指定する必要があった\n",
    "- 重みの初期化手法(He, Xavier)を選択する必要があった\n",
    "- 全結合層に対しては、平滑化した入力データが必要だった\n",
    "- ミニバッチサイズを指定する必要があった\n",
    "- 全結合層に対しては、レイヤー別にノード数を指定する必要があった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの用意\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n",
    "\n",
    "[Iris Species](https://www.kaggle.com/uciml/iris/data)\n",
    "\n",
    "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。\n",
    "\n",
    "- Iris-versicolor\n",
    "- Iris-virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
    "\n",
    "サンプルコード\n",
    "\n",
    "＊バージョン1.5から1.13の間で動作を確認済みです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:30.811786Z",
     "start_time": "2019-05-30T13:10:30.793836Z"
    }
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:30.952851Z",
     "start_time": "2019-05-30T13:10:30.938617Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(\n",
    "        layer_2, weights['w3']) + biases['b3']  # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:32.245050Z",
     "start_time": "2019-05-30T13:10:31.073139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 73.5804, val_loss : 46.8297, acc : 0.521, val_acc : 0.625\n",
      "Epoch 1, loss : 34.3071, val_loss : 30.2668, acc : 0.579, val_acc : 0.375\n",
      "Epoch 2, loss : 17.0055, val_loss : 3.3999, acc : 0.621, val_acc : 0.812\n",
      "Epoch 3, loss : 7.4064, val_loss : 2.7802, acc : 0.700, val_acc : 0.750\n",
      "Epoch 4, loss : 2.9025, val_loss : 1.5643, acc : 0.829, val_acc : 0.875\n",
      "Epoch 5, loss : 2.0979, val_loss : 2.2962, acc : 0.814, val_acc : 0.812\n",
      "Epoch 6, loss : 2.1418, val_loss : 0.6701, acc : 0.871, val_acc : 0.938\n",
      "Epoch 7, loss : 1.1022, val_loss : 0.0073, acc : 0.900, val_acc : 1.000\n",
      "Epoch 8, loss : 0.7134, val_loss : 0.5347, acc : 0.957, val_acc : 0.938\n",
      "Epoch 9, loss : 0.4282, val_loss : 0.2739, acc : 0.914, val_acc : 0.938\n",
      "Epoch 10, loss : 0.8457, val_loss : 0.5538, acc : 0.943, val_acc : 0.938\n",
      "Epoch 11, loss : 0.5477, val_loss : 0.5661, acc : 0.914, val_acc : 0.875\n",
      "Epoch 12, loss : 0.8901, val_loss : 0.6881, acc : 0.943, val_acc : 0.938\n",
      "Epoch 13, loss : 0.7505, val_loss : 0.6943, acc : 0.900, val_acc : 0.875\n",
      "Epoch 14, loss : 0.9388, val_loss : 0.8210, acc : 0.943, val_acc : 0.938\n",
      "Epoch 15, loss : 0.8577, val_loss : 1.6293, acc : 0.900, val_acc : 0.812\n",
      "Epoch 16, loss : 0.8432, val_loss : 2.3537, acc : 0.943, val_acc : 0.875\n",
      "Epoch 17, loss : 2.2722, val_loss : 8.7461, acc : 0.807, val_acc : 0.562\n",
      "Epoch 18, loss : 2.6667, val_loss : 2.0489, acc : 0.857, val_acc : 0.875\n",
      "Epoch 19, loss : 1.9237, val_loss : 6.1602, acc : 0.829, val_acc : 0.625\n",
      "test_acc : 0.650\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path = \"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\") |\n",
    "        (df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\",\n",
    "               \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y == 'Iris-versicolor'] = 0\n",
    "y[y == 'Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 20\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
    "                                 X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run(\n",
    "            [loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(\n",
    "            epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 重み  \n",
    "*tf.random_normal*を用いて正規分布からランダムな値を計算して初期化をし、*tf.Valiable*へデータフローグラフ構築時に値を持たせる。\n",
    "\n",
    "\n",
    "- エポック  \n",
    "スクラッチ同様、指定したエポック数に応じてループがされている\n",
    "\n",
    "\n",
    "- レイヤー（層）  \n",
    "レイヤーごとに層の種類、活性化関数(ex : tf.relu)を指定する\n",
    "\n",
    "\n",
    "- 学習率  \n",
    "スクラッチ同様、ハイパーパラメータとして指定\n",
    "\n",
    "\n",
    "- 最適化手法  \n",
    "*tf.train.AdamOptimizer*にてAdamアルゴリズムを指定し、minimizeにて最小化させる目的関数(tf.reduce_meanにて指定)を設定する\n",
    "\n",
    "\n",
    "- ミニバッチ  \n",
    "スクラッチ同様、ハイパーパラメータとして指定\n",
    "\n",
    "\n",
    "- ノード数  \n",
    "スクラッチ同様、ハイパーパラメータとして指定。重みとバイアスの宣言時に各レイヤーごとのノード数に応じて乱数初期化を行う。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "[Iris Species](https://www.kaggle.com/uciml/iris/data)\n",
    "\n",
    "サンプルコードと同じくこの中のtrain.csvを使用してください。目的変数はSpeciesに含まれる3種類全てを使います。\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。\n",
    "\n",
    "それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
    "\n",
    "ヒント\n",
    "\n",
    "以下の2箇所は2クラス分類特有の処理です。\n",
    "\n",
    "```python\n",
    "oss_op = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "```\n",
    "\n",
    "```python\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "```\n",
    "\n",
    "メソッドは以下のように公式ドキュメントを確認してください。\n",
    "\n",
    "[tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "[tf.math.sign  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/math/sign)\n",
    "\n",
    "＊tf.signとtf.math.signは同じ働きをします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:32.254737Z",
     "start_time": "2019-05-30T13:10:32.249045Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def onehot(y):\n",
    "    \"\"\"\n",
    "    多クラス分類を行う際のone-hot表現に変換\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_one_hot : 次の形のndarray, shape (n_samples, n_classes)\n",
    "        推定結果\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:33.289300Z",
     "start_time": "2019-05-30T13:10:32.258660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 46.3917, val_loss : 46.9633, acc : 0.467, val_acc : 0.583\n",
      "Epoch 1, loss : 14.7302, val_loss : 41.5780, acc : 0.657, val_acc : 0.583\n",
      "Epoch 2, loss : 12.4204, val_loss : 59.6991, acc : 0.617, val_acc : 0.292\n",
      "Epoch 3, loss : 11.7407, val_loss : 3.8696, acc : 0.720, val_acc : 0.583\n",
      "Epoch 4, loss : 12.0987, val_loss : 33.0661, acc : 0.743, val_acc : 0.583\n",
      "Epoch 5, loss : 10.2755, val_loss : 44.9036, acc : 0.767, val_acc : 0.583\n",
      "Epoch 6, loss : 7.5043, val_loss : 34.3293, acc : 0.737, val_acc : 0.583\n",
      "Epoch 7, loss : 5.4504, val_loss : 27.5288, acc : 0.797, val_acc : 0.583\n",
      "Epoch 8, loss : 3.2603, val_loss : 2.5207, acc : 0.870, val_acc : 0.833\n",
      "Epoch 9, loss : 2.1279, val_loss : 2.7349, acc : 0.860, val_acc : 0.875\n",
      "test_acc : 0.867\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを3値分類する\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path = \"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\",\n",
    "               \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y == 'Iris-versicolor'] = 0\n",
    "y[y == 'Iris-virginica'] = 1\n",
    "y[y == 'Iris-setosa'] = 2  # 追加\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 多クラス分類のため、ワンホットエンコーディングする\n",
    "y = onehot(y.reshape(-1))\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数(ソフトマックスクロスエントロピー)\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法(GradientDescentOptimizer : 最急降下法)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "# 多クラス分類のため、最大値のインデックスが一致しているかで推定する\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
    "                                 X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run(\n",
    "            [loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(\n",
    "            epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T12:23:23.787063Z",
     "start_time": "2019-05-29T12:23:23.779235Z"
    }
   },
   "source": [
    "## 【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:33.299422Z",
     "start_time": "2019-05-30T13:10:33.293974Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:33.352274Z",
     "start_time": "2019-05-30T13:10:33.303836Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:33.388242Z",
     "start_time": "2019-05-30T13:10:33.354449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの確認\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:37.375316Z",
     "start_time": "2019-05-30T13:10:33.391676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 1.1604, val_loss : 0.5633\n",
      "Epoch 1, loss : 0.6614, val_loss : 0.5062\n",
      "Epoch 2, loss : 0.5940, val_loss : 0.4787\n",
      "Epoch 3, loss : 0.5527, val_loss : 0.4595\n",
      "Epoch 4, loss : 0.5229, val_loss : 0.4449\n",
      "Epoch 5, loss : 0.5002, val_loss : 0.4332\n",
      "Epoch 6, loss : 0.4821, val_loss : 0.4235\n",
      "Epoch 7, loss : 0.4673, val_loss : 0.4143\n",
      "Epoch 8, loss : 0.4547, val_loss : 0.4062\n",
      "Epoch 9, loss : 0.4444, val_loss : 0.3980\n",
      "Epoch 10, loss : 0.4347, val_loss : 0.3909\n",
      "Epoch 11, loss : 0.4261, val_loss : 0.3848\n",
      "Epoch 12, loss : 0.4182, val_loss : 0.3783\n",
      "Epoch 13, loss : 0.4112, val_loss : 0.3726\n",
      "Epoch 14, loss : 0.4042, val_loss : 0.3665\n",
      "Epoch 15, loss : 0.3974, val_loss : 0.3616\n",
      "Epoch 16, loss : 0.3913, val_loss : 0.3573\n",
      "Epoch 17, loss : 0.3857, val_loss : 0.3534\n",
      "Epoch 18, loss : 0.3806, val_loss : 0.3499\n",
      "Epoch 19, loss : 0.3760, val_loss : 0.3463\n",
      "Epoch 20, loss : 0.3718, val_loss : 0.3428\n",
      "Epoch 21, loss : 0.3677, val_loss : 0.3391\n",
      "Epoch 22, loss : 0.3639, val_loss : 0.3358\n",
      "Epoch 23, loss : 0.3605, val_loss : 0.3323\n",
      "Epoch 24, loss : 0.3575, val_loss : 0.3291\n",
      "Epoch 25, loss : 0.3547, val_loss : 0.3265\n",
      "Epoch 26, loss : 0.3521, val_loss : 0.3236\n",
      "Epoch 27, loss : 0.3497, val_loss : 0.3211\n",
      "Epoch 28, loss : 0.3474, val_loss : 0.3189\n",
      "Epoch 29, loss : 0.3454, val_loss : 0.3168\n",
      "test_loss : 0.446\n"
     ]
    }
   ],
   "source": [
    "# 目的変数、説明変数の抽出\n",
    "X = df.loc[:, ['GrLivArea', 'YearBuilt']].values\n",
    "y = df.loc[:, 'SalePrice'].values\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 説明変数データの標準化\n",
    "ss_x = preprocessing.StandardScaler()\n",
    "ss_x.fit(X_train.astype(float))\n",
    "X_train_std = ss_x.transform(X_train.astype(float))\n",
    "X_val_std = ss_x.transform(X_val.astype(float))\n",
    "X_test_std = ss_x.transform(X_test.astype(float))\n",
    "\n",
    "# 目的変数データの標準化\n",
    "ss_y = preprocessing.StandardScaler()\n",
    "ss_y.fit(y_train.reshape(-1, 1).astype(float))\n",
    "y_train_std = ss_y.transform(y_train.reshape(-1, 1).astype(float))\n",
    "y_val_std = ss_y.transform(y_val.reshape(-1, 1).astype(float))\n",
    "y_test_std = ss_y.transform(y_test.reshape(-1, 1).astype(float))\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 30\n",
    "\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 5\n",
    "n_input = X_train_std.shape[1]\n",
    "n_samples = X_train_std.shape[0]\n",
    "# 回帰問題のため、クラス数は1とする\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train_std, y_train_std, batch_size)\n",
    "\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数(平均二乗誤差)\n",
    "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 回帰のため、精度は省略\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train_std.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={\n",
    "                     X: mini_batch_x, Y: mini_batch_y.reshape(-1, 1)})\n",
    "            loss = sess.run(loss_op, feed_dict={\n",
    "                            X: mini_batch_x, Y: mini_batch_y.reshape(-1, 1)})\n",
    "            total_loss += loss\n",
    "        total_loss /= total_batch\n",
    "        val_loss = sess.run(loss_op, feed_dict={\n",
    "                            X: X_val_std, Y: y_val_std.reshape(-1, 1)})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(\n",
    "            epoch, total_loss, val_loss))\n",
    "    test_loss = sess.run(loss_op, feed_dict={\n",
    "                         X: X_test_std, Y: y_test_std.reshape(-1, 1)})\n",
    "    print(\"test_loss : {:.3f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:37.385847Z",
     "start_time": "2019-05-30T13:10:37.379678Z"
    }
   },
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:37.847484Z",
     "start_time": "2019-05-30T13:10:37.392766Z"
    }
   },
   "outputs": [],
   "source": [
    "# mnistデータを読み込み\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:37.855923Z",
     "start_time": "2019-05-30T13:10:37.850477Z"
    }
   },
   "outputs": [],
   "source": [
    "# (n_samples, n_channels, height, width)のNCHW\n",
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:38.386122Z",
     "start_time": "2019-05-30T13:10:37.860257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# float型へ\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max())  # 1.0\n",
    "print(X_train.min())  # 0.0\n",
    "\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 1*28*28)\n",
    "X_test = X_test.reshape(-1, 1*28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:10:38.841317Z",
     "start_time": "2019-05-30T13:10:38.389216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "\n",
    "# ワンホットを追加\n",
    "y_train = onehot(y_train.reshape(-1))\n",
    "y_val = onehot(y_val.reshape(-1))\n",
    "y_test = onehot(y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:12:02.917342Z",
     "start_time": "2019-05-30T13:10:38.847078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 5.1805, val_loss : 1.2913, acc : 0.746, val_acc : 0.685\n",
      "Epoch 1, loss : 0.8403, val_loss : 1.1717, acc : 0.760, val_acc : 0.780\n",
      "Epoch 2, loss : 0.5672, val_loss : 0.4544, acc : 0.839, val_acc : 0.886\n",
      "Epoch 3, loss : 0.3424, val_loss : 0.3402, acc : 0.905, val_acc : 0.909\n",
      "Epoch 4, loss : 0.2838, val_loss : 0.3359, acc : 0.924, val_acc : 0.915\n",
      "Epoch 5, loss : 0.2568, val_loss : 0.3589, acc : 0.932, val_acc : 0.914\n",
      "Epoch 6, loss : 0.2380, val_loss : 0.3314, acc : 0.937, val_acc : 0.923\n",
      "Epoch 7, loss : 0.2361, val_loss : 0.3290, acc : 0.938, val_acc : 0.923\n",
      "Epoch 8, loss : 0.2289, val_loss : 0.3316, acc : 0.939, val_acc : 0.925\n",
      "Epoch 9, loss : 0.2239, val_loss : 0.3665, acc : 0.942, val_acc : 0.925\n",
      "test_acc : 0.930\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "# 0〜9の10クラス\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法(Admaアルゴリズムの方が精度良さそう)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
    "                                 X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run(\n",
    "            [loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(\n",
    "            epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
