{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint12課題 深層学習スクラッチ畳み込みニューラルネットワーク1\n",
    "\n",
    "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "Sprint11で作成したディープニューラルネットワークのクラスを拡張する形でCNNを作成します。まず、Sprint12で1次元畳み込み層を作成し、畳み込みの基礎を理解することを目指します。そして、Sprint13で一般的に画像に対して使われる2次元畳み込み層とプーリング層を作成します。\n",
    "\n",
    "### 1次元畳み込み層\n",
    "畳み込みニューラルネットワークは画像に対して使われる2次元畳み込みが代表的ですが、理解を容易にするためにまずは1次元畳み込みを実装します。1次元畳み込みは系列データで使われることが多いです。畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまでがフレームワークで一般的に用意されています。\n",
    "\n",
    "### データセットの用意\n",
    "引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。\n",
    "\n",
    "### CNN分類器クラスの作成\n",
    "1次元畳み込みニューラルネットワークモデルのクラスScratch1dCNNClassifierを作成してください。Sprint11で作成したScratchDeepNeuralNetrowkClassifierを元にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:40:11.214352Z",
     "start_time": "2019-06-26T13:40:11.207949Z"
    }
   },
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:04:48.298150Z",
     "start_time": "2019-06-26T12:04:40.174611Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:04:48.303655Z",
     "start_time": "2019-06-26T12:04:48.300426Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:04:49.457017Z",
     "start_time": "2019-06-26T12:04:48.308292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#float型へ\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "#正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:04:50.112382Z",
     "start_time": "2019-06-26T12:04:49.462700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "#データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造はsprint11で作成したFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここではパディングは考えず、ストライドも1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$に関する損失$L$の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$に関する損失$L$の勾配\n",
    "\n",
    "勾配$\\frac{\\partial L}{\\partial w_s}$ や$\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ :  勾配の配列のi番目の値\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$: 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "ただし、$j-s<0$  または$j-s>N_{out} - 1$のとき$\\frac{\\partial L}{\\partial a_{(j-s)}} =0$です  \n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:22:08.576865Z",
     "start_time": "2019-06-26T12:22:08.558218Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_filters : int\n",
    "        フィルター数\n",
    "    filter_w : int\n",
    "        フィルターの幅\n",
    "    optimizer : instance\n",
    "        最適化手法\n",
    "    stride : int\n",
    "        ストライド数\n",
    "    pad : int\n",
    "        パディング数\n",
    "    \n",
    "    Attribute\n",
    "    ------------\n",
    "    self.weight : ndarray, shape(n_filters, filter_w)\n",
    "        重み（フィルター）\n",
    "    self.bias : ndarray, shpae(n_channel)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters, filter_w, optimizer, stride, pad, w, b):\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.filter_w = filter_w\n",
    "        self.optimizer = optimizer\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.weight = w\n",
    "        self.bias = b\n",
    "        #self.weight = np.random.randn(n_filters, filter_w)\n",
    "        #self.bias = np.zeros(1)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, width)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (n_samples, n_filters, out_w)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #バックワード用に保存\n",
    "        self.forward_X = X.copy()\n",
    "        \n",
    "        #入力データのshape\n",
    "        self.n, self.in_w = X.shape\n",
    "\n",
    "        #1次元畳み込み後の出力サイズ\n",
    "        self.out_w = output_size(self.in_w, self.filter_w, self.stride, self.pad)\n",
    "\n",
    "        #出力格納用の空箱\n",
    "        A = np.zeros([self.n, self.n_filters, self.out_w])\n",
    "        \n",
    "        #1d_convlution\n",
    "        for sample in range(self.n):\n",
    "            #フィルター枚数分繰り返し\n",
    "            for fil in range(self.n_filters):\n",
    "                #横のストライド\n",
    "                for w_conv in range(self.out_w):\n",
    "                    #出力用の配列の対応要素に加算していく。\n",
    "                    A[sample][fil][w_conv] += np.sum(\n",
    "                        X[sample][w_conv:w_conv+self.filter_w] * self.weight[fil]) + self.bias\n",
    "                   \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (n_samples, n_filters, out_w)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (n_samples, width)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        #チャンネルを軸とした勾配合計\n",
    "        self.dB = np.sum(dA)\n",
    "        \n",
    "        #deltaWの空箱\n",
    "        self.dW = np.zeros(self.weight.shape)\n",
    "\n",
    "        #deltaXの空箱\n",
    "        dX = np.zeros([dA.shape[0], self.in_w])\n",
    "        \n",
    "        #サンプル数\n",
    "        for sample in range(dA.shape[0]):\n",
    "            #フィルター数\n",
    "            for filters in range(self.n_filters):\n",
    "                #横の出力サイズ\n",
    "                for w_conv in range(self.out_w):\n",
    "                    #横のフィルターサイズ\n",
    "                    for w_filter in range(self.filter_w):\n",
    "\n",
    "                        #deltaWの対応要素へ加算していく\n",
    "                        self.dW[filters][w_filter] +=\\\n",
    "                            dA[sample][filters][w_conv] * self.forward_X[sample][w_conv+w_filter]\n",
    "\n",
    "                        #deltaXの対応要素に加算していく\n",
    "                        dX[sample][w_conv+w_filter] +=\\\n",
    "                            dA[sample][filters][w_conv] * self.weight[filters][w_filter]\n",
    "\n",
    "                                    \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "                      \n",
    "        return dX\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:23:18.438384Z",
     "start_time": "2019-06-26T12:23:18.432311Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.weight -= self.lr * layer.dW\n",
    "        layer.bias -= np.mean(self.lr * layer.dB)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:23:18.888803Z",
     "start_time": "2019-06-26T12:23:18.868199Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_size(in_n, filter_w, stride, pad):\n",
    "    '''\n",
    "    畳み込み後の出力サイズを計算する\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_n : int\n",
    "        入力サイズ（特徴量の数）\n",
    "    filter_w : int\n",
    "        フィルタのサイズ\n",
    "    stride : int\n",
    "        ストライドのサイズ\n",
    "    pad : パディングのサイズ\n",
    "    \n",
    "    Return\n",
    "    ---------\n",
    "    out_n : int\n",
    "         出力のサイズ（特徴量の数）\n",
    "    \n",
    "    '''\n",
    "\n",
    "    out_n = int(((in_n + 2 * pad - filter_w) / stride) + 1)\n",
    "\n",
    "    return out_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。\n",
    "\n",
    "```python\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "```\n",
    "\n",
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "\n",
    "```python\n",
    "a = np.array([35, 50])\n",
    "```\n",
    "\n",
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "```python\n",
    "delta_a = np.array([10, 20])\n",
    "```\n",
    "\n",
    "バックプロパゲーションをすると次のような値になります。\n",
    "```python\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### フォワードプロパゲーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:38.965742Z",
     "start_time": "2019-06-26T12:25:38.959631Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3., 5., 7.])\n",
    "b = np.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:39.299063Z",
     "start_time": "2019-06-26T12:25:39.293647Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:39.633177Z",
     "start_time": "2019-06-26T12:25:39.625252Z"
    }
   },
   "outputs": [],
   "source": [
    "sc1d = SimpleConv1d(\n",
    "    n_filters=1, \n",
    "    filter_w=3, \n",
    "    optimizer=optimizer, \n",
    "    stride=1, \n",
    "    pad=0, \n",
    "    w=w.reshape(1,-1), \n",
    "    b=b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:40.030392Z",
     "start_time": "2019-06-26T12:25:40.008308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[35., 50.]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc1d.forward(x.reshape(1,-1))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:23:20.168826Z",
     "start_time": "2019-06-26T12:23:20.159921Z"
    }
   },
   "source": [
    "#### バックプロパゲーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:41.112991Z",
     "start_time": "2019-06-26T12:25:41.108715Z"
    }
   },
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:41.577020Z",
     "start_time": "2019-06-26T12:25:41.567268Z"
    }
   },
   "outputs": [],
   "source": [
    "delta_x = sc1d.backward(delta_a.reshape(1,1,2))\n",
    "delta_b = sc1d.dB\n",
    "delta_w = sc1d.dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T12:25:42.079926Z",
     "start_time": "2019-06-26T12:25:42.071542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b : 30\n",
      "delta_w : [[ 50.  80. 110.]]\n",
      "delta_x : [[ 30. 110. 170. 140.]]\n"
     ]
    }
   ],
   "source": [
    "print('delta_b : {}'.format(delta_b))\n",
    "print('delta_w : {}'.format(delta_w))\n",
    "print('delta_x : {}'.format(delta_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "紙やホワイトボードを使い計算グラフを書きながら考えてください。\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "\n",
    "```python\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "```\n",
    "\n",
    "出力は次のようになります。\n",
    "```python\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
    "```\n",
    "\n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "**補足**\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:41:50.142424Z",
     "start_time": "2019-06-26T13:41:50.119622Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_filters : int\n",
    "        フィルター数\n",
    "    n_channel : int\n",
    "        チャンネル数\n",
    "    filter_w : int\n",
    "        フィルターの幅\n",
    "    optimizer : instance\n",
    "        最適化手法\n",
    "    stride : int\n",
    "        ストライド数\n",
    "    pad : int\n",
    "        パディング数\n",
    "    \n",
    "    Attribute\n",
    "    ------------\n",
    "    self.weight : ndarray, shape(n_filters, n_channel, filter_w)\n",
    "        重み（フィルター）\n",
    "    self.bias : ndarray, shpae(n_channel)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters, n_channel, filter_w, optimizer, stride, pad, w=None, b=None):\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.n_channel = n_channel\n",
    "        self.filter_w = filter_w\n",
    "        self.optimizer = optimizer\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        if w is None:\n",
    "            self.weight = np.random.randn(n_filters, n_channel, filter_w)\n",
    "        else:\n",
    "            self.weight = w\n",
    "            \n",
    "        if b is None:\n",
    "            self.bias = np.zeros(n_channel)\n",
    "        else:\n",
    "            self.bias = b\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_channels, width)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (n_samples, n_filters, out_w)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #バックワード用に保存\n",
    "        self.forward_X = X.copy()\n",
    "        \n",
    "        #入力データのshape\n",
    "        self.n, self.in_c, self.in_w = X.shape\n",
    "\n",
    "        #1次元畳み込み後の出力サイズ\n",
    "        self.out_w = output_size(self.in_w, self.filter_w, self.stride, self.pad)\n",
    "\n",
    "        #出力格納用の空箱\n",
    "        A = np.zeros([self.n, self.n_filters, self.out_w])\n",
    "        \n",
    "        #1d_convlution\n",
    "        for sample in range(self.n):\n",
    "            #フィルター枚数分繰り返し\n",
    "            for fil in range(self.n_filters):\n",
    "                #チャンネル数分繰り返し\n",
    "                for channel in range(self.in_c):\n",
    "                    #横のストライド\n",
    "                    for w_conv in range(self.out_w):\n",
    "                        #出力用の配列の対応要素に加算していく。\n",
    "                        A[sample][fil][w_conv] += np.sum(\n",
    "                            X[sample][channel][w_conv:w_conv+self.filter_w] \n",
    "                            * self.weight[fil][channel]) \n",
    "            A += self.bias.reshape(1, -1, 1)\n",
    "                   \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (n_samples, n_filters, out_w)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (n_samples, n_channels, width)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        #チャンネルを軸とした勾配合計\n",
    "        self.dB = np.sum(dA, axis=(0,2))\n",
    "        \n",
    "        #deltaWの空箱\n",
    "        self.dW = np.zeros(self.weight.shape)\n",
    "\n",
    "        #deltaXの空箱\n",
    "        dX = np.zeros([dA.shape[0], self.n_channel, self.in_w])\n",
    "        \n",
    "        #サンプル数\n",
    "        for sample in range(dA.shape[0]):\n",
    "            #チャンネル数\n",
    "            for channel in range(self.n_channel):\n",
    "                #フィルター数\n",
    "                for filters in range(self.n_filters):\n",
    "                    #横の出力サイズ\n",
    "                    for w_conv in range(self.out_w):\n",
    "                        #横のフィルターサイズ\n",
    "                        for w_filter in range(self.filter_w):\n",
    "\n",
    "                            #deltaWの対応要素へ加算していく\n",
    "                            self.dW[filters][channel][w_filter] +=\\\n",
    "                                dA[sample][filters][w_conv] *\\\n",
    "                                self.forward_X[sample][channel][w_conv+w_filter]\n",
    "\n",
    "                            #deltaXの対応要素に加算していく\n",
    "                            dX[sample][channel][w_conv+w_filter] +=\\\n",
    "                                dA[sample][filters][w_conv] *\\\n",
    "                                self.weight[filters][channel][w_filter]\n",
    "\n",
    "                                    \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "                      \n",
    "        return dX\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:41:50.834037Z",
     "start_time": "2019-06-26T13:41:50.828585Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1., 2., 3.]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:41:51.483019Z",
     "start_time": "2019-06-26T13:41:51.478305Z"
    }
   },
   "outputs": [],
   "source": [
    "conv1d = Conv1d(\n",
    "    n_filters=3, \n",
    "    n_channel=2, \n",
    "    filter_w=3, \n",
    "    optimizer=optimizer, \n",
    "    stride=1, \n",
    "    pad=0, \n",
    "    w=w, \n",
    "    b=b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:41:52.447968Z",
     "start_time": "2019-06-26T13:41:52.434957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[16., 22.],\n",
       "        [17., 23.],\n",
       "        [18., 24.]]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = conv1d.forward(x.reshape(1,2,4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:41:55.524993Z",
     "start_time": "2019-06-26T13:41:55.514661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 51., 120., 120.,  69.],\n",
       "        [ 51., 120., 120.,  69.]]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.backward(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】学習・推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えて学習と推定を行ってください。出力層だけは全結合層をそのまま使ってください。\n",
    "\n",
    "チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、平滑化を行います。平滑化はNumPyのreshapeが使用できます。\n",
    "\n",
    "[numpy.reshape — NumPy v1.15 Manual](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.reshape.html)\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:50:57.716829Z",
     "start_time": "2019-06-26T13:50:57.706127Z"
    }
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \n",
    "    Attribute\n",
    "    ------------\n",
    "    self.W : 重み\n",
    "    self.B : バイアス\n",
    "    self.H_w : 前のイテレーションまでの勾配の(重み)二乗和(初期値0)\n",
    "    self.H_b : 前のイテレーションまでの勾配の(バイアス)二乗和(初期値0)\n",
    "    self.forward_Z : forward時の入力値(backward用に利用)\n",
    "    self.dW : 重みの勾配\n",
    "    self.dB : バイアスの勾配\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.weight = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.bias = initializer.B(n_nodes2)\n",
    "                \n",
    "        self.forward_Z=None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #backfoward用に保存\n",
    "        self.forward_Z = Z.copy()\n",
    "        \n",
    "        A = (Z @ self.weight) + self.bias\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = dA.copy()\n",
    "        self.dW = self.forward_Z.T @ dA\n",
    "        \n",
    "        #[batch_size, n_nodes2] dot [n_nodes2, n_nodes1]\n",
    "        #→ [batch_size, n_nodes1]\n",
    "        dZ = dA @ self.weight.T \n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "              \n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:50:57.979033Z",
     "start_time": "2019-06-26T13:50:57.962253Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool1D:\n",
    "    '''\n",
    "    最大プーリング層\n",
    "    Parameters\n",
    "    ----------\n",
    "    stride_w : int\n",
    "        横のストライドサイズ\n",
    "    \n",
    "    Attribute\n",
    "    -------------\n",
    "    self.forwad_X : ndarray, shape (n_samples, n_channels, width)\n",
    "        フォワード時の入力値データ（バックワード用）\n",
    "    self.out_w : int\n",
    "        プーリング時の出力時の幅\n",
    "    self.pool_out_idx :  ndarray, shape (n_samples, n_filters, out_w)\n",
    "        プーリング時の最大値インデックス\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, stride_w):\n",
    "        \n",
    "        self.stride_w = stride_w\n",
    "    \n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        プーリングを行い、ダウンサンプリングする\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_channels, width)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (n_samples, n_filters, out_w)\n",
    "            出力\n",
    "        '''  \n",
    "        #バックワード用にコピー\n",
    "        self.forward_X = X.copy()\n",
    "        \n",
    "        #入力データのshape\n",
    "        N, C , in_w = X.shape\n",
    "\n",
    "        #出力サイズの計算\n",
    "        self.out_w = int(in_w/self.stride_w)\n",
    "\n",
    "        #出力データ(プーリング結果)の空箱\n",
    "        A = np.zeros([N, C, self.out_w])\n",
    "        #プーリング時の最大値インデックス記憶する空箱\n",
    "        self.pool_out_idx = np.zeros([N, C, self.out_w])\n",
    "        \n",
    "        for sample in range(N):\n",
    "            for channel in range(C):\n",
    "                for w_pool in range(self.out_w):\n",
    "\n",
    "                    #プーリング結果を出力配列に更新していく\n",
    "                    A[sample][channel][w_pool] = \\\n",
    "                        np.max(X[sample][channel]\n",
    "                               [w_pool*self.stride_w:(w_pool+1)*self.stride_w])\n",
    "\n",
    "                    #プーリング時の最大値インデックスを記憶していく\n",
    "                    self.pool_out_idx[sample][channel][w_pool] = \\\n",
    "                        np.argmax(X[sample][channel]\n",
    "                                  [w_pool*self.stride_w:(w_pool+1)*self.stride_w])\n",
    "                        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        '''\n",
    "        プーリングのバックワード、アップサンプリングする\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (n_samples, n_filters, out_w)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dX : 次の形のndarray, shape (n_samples, n_channels, width)\n",
    "            前に流す勾配\n",
    "        '''\n",
    "        #入力データのサンプル数、チャンネル数を確認\n",
    "        N = dA.shape[0]\n",
    "        C = dA.shape[1]\n",
    "        \n",
    "        #出力データの空箱を作成する\n",
    "        dX = np.zeros(self.forward_X.shape)\n",
    "\n",
    "        for sample in range(N):\n",
    "            for channel in range(C):\n",
    "                for w_pool in range(self.out_w):\n",
    "\n",
    "                    #最大値インデックスより、ストライド配列内の列を特定する\n",
    "                    column = int(self.pool_out_idx[sample][channel][w_pool])\n",
    "\n",
    "                    #フォワード前と同じインデックスのみに対して更新を行う\n",
    "                    dX[sample][channel][w_pool*self.stride_w+column]\\\n",
    "                        = dA[sample][channel][w_pool]\n",
    "    \n",
    "        \n",
    "        \n",
    "        return dX\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:50:58.111762Z",
     "start_time": "2019-06-26T13:50:58.104357Z"
    }
   },
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    \"\"\"\n",
    "    ReLUの計算\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    Attribute\n",
    "    -----------\n",
    "    self.mask : 入力値の0以下を判定するboolリスト\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mask = None     \n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードにおける活性化関数の計算\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        A : 活性化関数計算前\n",
    "        \n",
    "        Return\n",
    "        -----------\n",
    "        Z : 出力\n",
    "\n",
    "        \"\"\"\n",
    "        self.mask = (A <= 0)\n",
    "        \n",
    "        Z = A.copy()\n",
    "        \n",
    "        Z[self.mask] = 0\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワードにおける活性化関数の計算\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        dZ : 活性化関数計算前\n",
    "        \n",
    "        Return\n",
    "        -----------\n",
    "        dA : 出力\n",
    "\n",
    "        \"\"\"        \n",
    "        dA = dZ.copy()\n",
    "        \n",
    "        dA[self.mask] = 0\n",
    "        \n",
    "            \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:50:58.337237Z",
     "start_time": "2019-06-26T13:50:58.324997Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    softmaxの計算\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    Attribute\n",
    "    -----------\n",
    "    self.cost : 交差エントロピー誤差を格納\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cost = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードにおけるソフトマックスの計算\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        A : 活性化関数計算前\n",
    "        \n",
    "        Return\n",
    "        -----------\n",
    "        Z : 出力\n",
    "        \"\"\"\n",
    "        #オーバーフロー対策\n",
    "        max_A = np.max(A)\n",
    "\n",
    "        #最大要素を引いてからexpをかけることでオーバーフローを回避\n",
    "        exp_A = np.exp(A - max_A)\n",
    "\n",
    "         #分母を計算\n",
    "        sum_exp_A = np.sum(exp_A, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        Z = exp_A / sum_exp_A\n",
    "\n",
    "        return Z\n",
    "        \n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        \"\"\"\n",
    "        バックワードにおけるソフトマックスと交差エントロピー誤差\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        Z : 出力層で計算された出力\n",
    "        Y : 正解値\n",
    "        \n",
    "        Return\n",
    "        -----------\n",
    "        dA : 出力\n",
    "\n",
    "        \"\"\"\n",
    "        #交差エントロピー誤差\n",
    "        self.cost = - np.sum(Y * np.log(Z), axis=1)\n",
    "\n",
    "        #バックワード(出力層)\n",
    "        dA = Z - Y\n",
    "        \n",
    "        return dA\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:50:58.466180Z",
     "start_time": "2019-06-26T13:50:58.456226Z"
    }
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1) * np.sqrt(2)\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T13:50:58.621524Z",
     "start_time": "2019-06-26T13:50:58.613561Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    '''\n",
    "    平滑化を行う\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    Attribute\n",
    "    -------------\n",
    "    self.f_c : int\n",
    "        フォワード時の入力データチャンネル数\n",
    "    self.f_w : int\n",
    "        フォワード時の入力データ幅\n",
    "    '''\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.f_c = None\n",
    "        self.f_w = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        入力データに対して平滑化を行い、3次元→2次元にする\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_channels, width)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (n_samples, n_channels * width)\n",
    "            出力\n",
    "        '''  \n",
    "        #入力データの形\n",
    "        f_n, self.f_c, self.f_w = X.shape\n",
    "        \n",
    "        fX = X.reshape(f_n, self.f_c * self.f_w)\n",
    "        \n",
    "        return fX\n",
    "        \n",
    "        \n",
    "    def backward(self, fX):\n",
    "        '''\n",
    "        フォワードにて平滑化された入力値のshapeを元に戻す\n",
    "        Parameters\n",
    "        ----------\n",
    "        fxX : 次の形のndarray, shape (n_samples, n_channels * width)\n",
    "            平滑化された入力値\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (n_samples, n_channels, width)\n",
    "            出力（元に戻す）\n",
    "        '''  \n",
    "        #サンプル数を確認\n",
    "        b_n = fX.shape[0]\n",
    "        \n",
    "        #元の形に戻す\n",
    "        rX = fX.reshape(b_n, self.f_c, self.f_w)\n",
    "        \n",
    "        return rX\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:09:02.845595Z",
     "start_time": "2019-06-26T14:09:02.794488Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier():\n",
    "    \"\"\"\n",
    "    1次元畳み込みニューラルネットワーク\n",
    "    (sgd)\n",
    "    conv2d - relu - pooling - relu - FC - softmax\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "        学習過程を出力する場合はTrue\n",
    "    n_epochs : int(default:30)\n",
    "        イテレーション数\n",
    "    lr : flaot (default:1e-3)\n",
    "        学習率\n",
    "    batch : int(default :10)\n",
    "        ミニバッチの単位数\n",
    "    conv_n_filters : int(default:3)\n",
    "        畳み込み時のフィルター枚数\n",
    "    conv_n_channel : int(default:1)\n",
    "        畳み込み時のチャンネル数\n",
    "    conv_filter_w : int(default:3)\n",
    "        畳み込み時のフィルターサイズ(横)\n",
    "    conv_stride : int(default:1)\n",
    "        畳み込み時のストライド数\n",
    "    conv_pad : int(default:0)\n",
    "        畳み込み時のパディング数\n",
    "    pool_stride_w : int(default:2)\n",
    "        プーリング時のストライドサイズ(横)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.loss : ndarray,shape(n_epochs, )\n",
    "        エポックごとの誤差を格納\n",
    "    self.val_loss : ndarray,shape(n_epochs, )\n",
    "        エポックごとの誤差(検証用データ)を格納\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose = True, n_epochs=10, lr=1e-3, batch=10, \n",
    "                 conv_n_filters=3, conv_n_channel=1, conv_filter_w=3, \n",
    "                 conv_stride=1, conv_pad=0, pool_stride_w=2):\n",
    "\n",
    "        self.verbose = verbose                            #True(default):学習過程を表示、False:非表示\n",
    "        self.n_epochs = n_epochs                       #エポック数(default:30)\n",
    "        self.lr = lr                                               #学習率(default:1e-3)\n",
    "        self.batch = batch                                  #ミニバッチを行うサイズ(default:10)\n",
    "        self.conv_n_filters = conv_n_filters          #畳み込み時のフィルター枚数\n",
    "        self.conv_n_channel = conv_n_channel   #畳み込み時のチャンネル数\n",
    "        self.conv_filter_w = conv_filter_w           #畳み込み時のフィルターサイズ(横)\n",
    "        self.conv_stride = conv_stride               #畳み込み時ストライド数\n",
    "        self.conv_pad = conv_pad                     #畳み込み時のパディング数\n",
    "        self.pool_stride_w = pool_stride_w         #プーリング時のストライド数(横)\n",
    "\n",
    "        self.loss = np.zeros(n_epochs)\n",
    "        self.val_loss = np.zeros(n_epochs)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        #出力層のアウトプットノード数\n",
    "        self.n_output = np.unique(y).shape[0]\n",
    "        \n",
    "        #conv後の出力サイズ\n",
    "        conv_out_w = int(((X.shape[2] + 2 * self.conv_pad - self.conv_filter_w) // self.conv_stride) + 1)\n",
    "        \n",
    "        #pooling後の出力サイズ\n",
    "        pool_out_w = int(conv_out_w // self.pool_stride_w)\n",
    "        \n",
    "        #出力層への入力サイズ\n",
    "        n_node = self.conv_n_filters * pool_out_w \n",
    "        \n",
    "        #正解データをワンホットエンコーディング\n",
    "        y = self._onehot(y)\n",
    "        #検証用データもあればワンホット\n",
    "        if X_val is not None:\n",
    "            y_val = self._onehot(y_val)\n",
    "\n",
    "\n",
    "        #入力層のノード数\n",
    "        self.n_features = X.shape[1]\n",
    "\n",
    "        #minibatchデータを生成\n",
    "        train_minibatch = GetMiniBatch(X, y, batch_size=self.batch)\n",
    "        \n",
    "\n",
    "        #最適化手法のインスタンスを選択、生成。\n",
    "        optimizer = SGD(self.lr)\n",
    "        \n",
    "        #活性化関数\n",
    "        \n",
    "        #1層目(conv1d)\n",
    "        #インスタンス生成、重みの初期化\n",
    "        self.conv1d = Conv1d(self.conv_n_filters, self.conv_n_channel, \n",
    "                         self.conv_filter_w, optimizer, self.conv_stride, self.conv_pad)\n",
    "        self.activation1 = Relu()\n",
    "\n",
    "        #pooling\n",
    "        #インスタンス生成\n",
    "        self.pool1d= MaxPool1D(self.pool_stride_w)\n",
    "        self.activation2 = Relu()\n",
    "\n",
    "        #3層目(全結合　出力層)\n",
    "        #インスタンス生成、重みの初期化\n",
    "        self.FC = FC(n_node, self.n_output, HeInitializer(), optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        #平滑化インスタンス生成\n",
    "        self.fl = Flatten()\n",
    "\n",
    "        #エポック数だけトレーニングを繰り返す\n",
    "        for epoch in range(self.n_epochs):\n",
    "                            \n",
    "            #引数で設定したminibatch数の単位で学習を行う\n",
    "            for mini_X, mini_y in train_minibatch:\n",
    "                X = mini_X.copy()\n",
    "                Y = mini_y.copy()\n",
    "                                            \n",
    "                #フォワードプロパゲーション\n",
    "                A1 = self.conv1d.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.pool1d.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                Z2 = self.fl.forward(Z2)\n",
    "                A3 = self.FC.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                                \n",
    "                #バックプロパゲーション\n",
    "                dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC.backward(dA3)\n",
    "                dZ2 = self.fl.backward(dZ2)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.pool1d.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.conv1d.backward(dA1) # dZ0は使用しない\n",
    "                    \n",
    "            ############\n",
    "            # 評価\n",
    "            ############\n",
    "            #誤差を格納\n",
    "            self.loss[epoch] = np.mean(self.activation3.cost)\n",
    "                        \n",
    "            #検証用データが引数にある場合、処理を行う\n",
    "            if X_val is not None:\n",
    "                                \n",
    "                #フォワードプロパゲーション\n",
    "                A1_val = self.conv1d.forward(X_val)\n",
    "                Z1_val = self.activation1.forward(A1_val)\n",
    "                A2_val = self.pool1d.forward(Z1_val)\n",
    "                Z2_val = self.activation2.forward(A2_val)\n",
    "                Z2_val = self.fl.forward(Z2_val)\n",
    "                A3_val = self.FC.forward(Z2_val)\n",
    "                Z3_val = self.activation3.forward(A3_val)\n",
    "                \n",
    "                #検証用データの交差エントロピー誤差を計算\n",
    "                cost_val = self._compute_cost(y_val, Z3_val)\n",
    "\n",
    "                #誤差を格納\n",
    "                self.val_loss[epoch] = np.mean(cost_val)\n",
    "                            \n",
    "\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                #一度だけ、'Cross Entropy Error'を出力\n",
    "                if epoch == 0:\n",
    "                    print('Cross Entropy Error')\n",
    "                    \n",
    "                #エポックごとのコスト関数を出力\n",
    "                print('epoch{} : {}'.format(epoch+1, np.mean(self.activation3.cost)))\n",
    "                \n",
    "                #検証用データがある場合、そのコスト関数も出力\n",
    "                if X_val is not None:\n",
    "                    print('epoch_val{} : {}'.format(epoch+1, np.mean(cost_val)))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred :  次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"        \n",
    "        #フォワードプロパゲーション\n",
    "        A1 = self.conv1d.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.pool1d.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        \n",
    "        #出力層の確率から、最大値をそのクラスとする\n",
    "        y_pred = np.argmax(Z3, axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def _onehot(self, y):\n",
    "        \"\"\"\n",
    "        多クラス分類を行う際のone-hot表現に変換\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_one_hot : 次の形のndarray, shape (n_samples, n_classes)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        return y_one_hot\n",
    "    \n",
    "    \n",
    "    #交差エントロピー誤差\n",
    "    def _compute_cost(self, y, y_pred):\n",
    "\n",
    "        return - np.sum(y * np.log(y_pred), axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:09:03.061139Z",
     "start_time": "2019-06-26T14:09:03.055838Z"
    }
   },
   "outputs": [],
   "source": [
    "s1dcnn = Scratch1dCNNClassifier(\n",
    "    verbose = True, \n",
    "    n_epochs=10, \n",
    "    lr=1, \n",
    "    batch=10, \n",
    "    conv_n_filters=1, \n",
    "    conv_n_channel=1, \n",
    "    conv_filter_w=3, \n",
    "    conv_stride=1, \n",
    "    conv_pad=0, \n",
    "    pool_stride_w=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:10:27.806532Z",
     "start_time": "2019-06-26T14:09:03.259546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Error\n",
      "epoch1 : 2.302585092994046\n",
      "epoch_val1 : 2.3025850929940455\n",
      "epoch2 : 2.302585092994046\n",
      "epoch_val2 : 2.3025850929940455\n",
      "epoch3 : 2.302585092994046\n",
      "epoch_val3 : 2.3025850929940455\n",
      "epoch4 : 2.302585092994046\n",
      "epoch_val4 : 2.3025850929940455\n",
      "epoch5 : 2.302585092994046\n",
      "epoch_val5 : 2.3025850929940455\n",
      "epoch6 : 2.302585092994046\n",
      "epoch_val6 : 2.3025850929940455\n",
      "epoch7 : 2.302585092994046\n",
      "epoch_val7 : 2.3025850929940455\n",
      "epoch8 : 2.302585092994046\n",
      "epoch_val8 : 2.3025850929940455\n",
      "epoch9 : 2.302585092994046\n",
      "epoch_val9 : 2.3025850929940455\n",
      "epoch10 : 2.302585092994046\n",
      "epoch_val10 : 2.3025850929940455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Scratch1dCNNClassifier at 0x1a17a12550>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#処理に時間がかかるため、入力データサンプル数を制限\n",
    "s1dcnn.fit(X_train.reshape(-1,1,784)[:300], y_train[:300],X_val.reshape(-1,1,784)[0:100], y_val[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※精度が上がらず。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングを加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "最も単純なパディングは全て0で埋めるゼロパディングであり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。\n",
    "\n",
    "なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "numpy.pad — NumPy v1.15 Manual\n",
    "\n",
    "**※省略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。\n",
    "\n",
    "**問題5にて実装済み**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。\n",
    "\n",
    "**問題5にて実装済み**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
