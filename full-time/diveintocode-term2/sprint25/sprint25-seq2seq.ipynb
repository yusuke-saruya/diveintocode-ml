{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint25課題 Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】機械翻訳の実行とコードリーディング\n",
    "Keras公式のサンプルコードで、短い英語からフランス語への変換が行えるのでこれを動かしてください。\n",
    "\n",
    "[keras/lstm_seq2seq.py at master · keras-team/keras](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n",
    "\n",
    "その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめてください。以下のようにどこからどこの行が何をしているかを記述してください。\n",
    "\n",
    "（例）\n",
    "\n",
    "- 51から55行目 : ライブラリのimport\n",
    "- 57から62行目 : ハイパーパラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:17:16.771281Z",
     "start_time": "2019-07-15T01:17:04.105745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインポート\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:23:03.481614Z",
     "start_time": "2019-07-15T01:18:07.789753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "WARNING:tensorflow:From /Users/yusuke-saruya/.pyenv/versions/anaconda3-2018.12/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/yusuke-saruya/.pyenv/versions/anaconda3-2018.12/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/yusuke-saruya/.pyenv/versions/anaconda3-2018.12/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.9216 - val_loss: 0.9535\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.7366 - val_loss: 0.7659\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.6223 - val_loss: 0.6807\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.5645 - val_loss: 0.6366\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.5243 - val_loss: 0.5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusuke-saruya/.pyenv/versions/anaconda3-2018.12/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "batch_size = 64  # 学習時のバッチサイズ\n",
    "epochs = 5  # 学習実のエポック数\n",
    "latent_dim = 256  # エンコーダの隠れ層の次元\n",
    "num_samples = 10000  # 学習データのサンプル数\n",
    "\n",
    "# データのパス\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# データをベクトル化する\n",
    "# テキスト格納用のlist, taple\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# データの読み込み\n",
    "# [英語 フランス語]の単語、文章\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "# num_sampleまたはデータ数に応じて学習を行う\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    \n",
    "    #英語をinput_textへ、フランス語をtarget_textへ\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # target_textに対して、start sequenceとしてtab('\\t')、\n",
    "    #  \"end sequence\" として\"\\n\"を使用する。\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    #listに格納する\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # textからユニーク文字列をリストに格納していく\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# 文字列リストをソート\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# 文字列の数をエンコーダ、デコーダーのtokenにする\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# textの中で一番大きい文字数を測る\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "#　いろいろとプリント\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# 文字列にインデックスを付与する\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# エンコーダ入力データ、デコーダー入力データ・ターゲットデータの初期化\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 入力テキストとターゲットテキスト\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # テキスト内の文字列と一致するインデックスを1にする\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_dataはdecoder_input_dataより1ステップさきに進んでいる\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # ここでdecoder_target_dataが1ステップ先に進み、\n",
    "            # startの文字列は含まないようにする\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# 入力データのシーケンスを定義して、エンコーディング処理を行う\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# `encoder_outputs`を破棄し、encoder_statesのみを保持する。\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 初期状態として `encoder_states`を使ってデコーダーを設定する。\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "\n",
    "# 内部statesを返すのと同様、出力シーケンスを返したデコーダーを設定する\n",
    "# この戻り値は学習モデルには使用しないが、推論時に使用する。\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# `encoder_input_data`と` decoder_input_data`を \n",
    "# `decoder_target_data`に変換するモデルを定義\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 学習\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# モデルを保存\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next：推論モード（サンプリング）\n",
    "#1）入力をエンコードしてstatesを取得する\n",
    "#2）statesとターゲットの\"start of sequence\"トークンでデコーダを1ステップ実行する\n",
    "#　　　出力は次のターゲットトークンになる\n",
    "#3）現在のターゲットトークンと現在の状態で繰り返す\n",
    "\n",
    "# サンプリングモデルの定義をする\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 隠れ層の次元数分の入力を設定する(hとCの分)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# デコーダの入力とそのstatesより、デコーダーのstatesを設定する\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# モデルに渡す\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# 時系列データを読み取り可能なものにするための逆引きトークンインデックス。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:23:06.045490Z",
     "start_time": "2019-07-15T01:23:03.484557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui est chait ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Sois cous en cherte ?\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Artene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: At chent !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: At chent !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Atrenez l'ent la !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Atrenez l'ent la !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Atrenez l'ent la !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Qui l'ait ?\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: At chent !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: At chent !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Artene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Artene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Atrene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Atrene le moit !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je me suis pas chent.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Arrête le moit !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: At-te le me cher ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: C'est pas ?\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous avons pas !\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons pas en chait.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons pas en chait.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons pas en chait.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons pas en chait.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Pous ent me lais ?\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Sois cous en cherte ?\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Soyez paste !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois chente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois chente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois chente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois chente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois chente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois chente !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois paste !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez pastente !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Artende le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Arrête le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Arrête le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Arrête le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Arrête le moit !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Atrenez l'ent !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Atrenez l'ent !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Atrenez l'ent !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 入力を状態ベクトルとしてエンコードする\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 長さ1の空のターゲット時系列データを生成\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # ターゲットシーケンスの最初の文字に開始文字'\\t'を入力\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # シーケンスのバッチ数分サンプルデータをループする\n",
    "    # (簡単にするためにここでは、サイズ1のバッチを想定する)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        #ターゲットシーケンスとエンコードしたstateからデコードする\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # トークンをサンプリング\n",
    "        #デコードした状態から最大値のインデックスを取得\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # 逆引きしたトークンのインデックスから文字を取得\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        \n",
    "        # デコードされた文章に追加する\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 文の文字数が最大に達するか、または停止文字があった場合、ループ停止\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 長さ1のターゲットシーケンスを更新する\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # statesを更新する\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # デコーディングを試すために学習データの一部からシーケンスを一つ取り出す\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:08:10.395736Z",
     "start_time": "2019-06-28T07:08:10.390340Z"
    }
   },
   "source": [
    "## 3.イメージキャプショニング\n",
    "他の活用例としてイメージキャプショニングがあります。画像に対する説明の文章を推定するタスクです。これは画像を入力し、時系列を出力するImage to Sequenceの手法によって行えます。\n",
    "\n",
    "[pytorch-tutorial/tutorials/03-advanced/image_captioning at master · yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n",
    "\n",
    "イメージキャプショニングは学習に多くの時間がかかるため、ここでは学習済みの重みが公開されている実装を動かすことにします。Kerasには平易に扱える実装が公開されていないため、今回はPyTorchによる実装を扱います。\n",
    "\n",
    "## 【問題2】イメージキャプショニングの学習済みモデルの実行\n",
    "上記実装において5. Test the modelの項目を実行してください。また、自身で用意した画像に対しても文章を生成してください。これらに対してどういった文章が出力されたかを記録して提出してください。\n",
    "\n",
    "データセットからの学習は行わず、学習済みの重みをダウンロードして利用します。\n",
    "\n",
    "注意点として、デフォルトで設定されている重みのファイル名と、ダウンロードできる重みのファイル名は異なっています。ここは書き換える必要があります。\n",
    "\n",
    "\n",
    "\n",
    "- example  \n",
    "![example](png/example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:23:24.961316Z",
     "start_time": "2019-07-15T01:23:06.049181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> a group of giraffes standing next to each other . <end>\r\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --image='png/example.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- testimage\n",
    "![testimage](png/testimage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:23:40.577190Z",
     "start_time": "2019-07-15T01:23:24.969566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> a group of people sitting around a table with a cake . <end>\r\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --image='png/testimage.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自身で用意した画像に対しても、大勢の人、ケーキのおもちゃを捉えるなど、おおよその状況を表した文章を返すことができた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】Kerasで動かしたい場合はどうするかを調査\n",
    "PyTorchによる実装を動かしましたが、何らかの理由からKerasで動かしたい状況が考えられます。どういった手順を踏むことになるか調査し、できるだけ詳しく説明してください。\n",
    "\n",
    "特に今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### １．学習済みモデルをPyTorchからKerasに変換させる\n",
    "- MMdnn(Microsoft Researchにより開発が進められているオープンソースの深層学習モデルの変換と可視化を行うツール)を利用して学習済みモデルの変換を行う。その際、MMdnnではアーキテクチャと重みが含まれるモデルを変換することができるが、重みしか含まれていないモデルは変換することができない。\"AttributeError: 'collections.OrderedDict' object has no attribute 'state_dict'\"\n",
    "\n",
    "### ２．コードの書き換え\n",
    "- PyTorchにより実装されているモデル構築から推測までのコードをKerasを利用して書き換える。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】（アドバンス課題）コードリーディングと書き換え\n",
    "モデル部分はmodel.pyに書かれていますが、Kerasではこのモデルがどのように記述できるかを考え、コーディングしてください。その際機械翻訳のサンプルコードが参考になります。\n",
    "\n",
    "**※省略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】（アドバンス課題）発展的調査\n",
    "**他の言語の翻訳を行う場合は？**\n",
    "\n",
    "問題1の実装を使い日本語と英語の翻訳を行いたい場合はどのような手順を踏むか考えてみましょう。\n",
    "\n",
    "**機械翻訳の発展的手法にはどのようなものがある？**\n",
    "\n",
    "機械翻訳のための発展的手法にはどういったものがあるか調査してみましょう。\n",
    "\n",
    "**文章から画像生成するには？**\n",
    "\n",
    "イメージキャプショニングとは逆に文章から画像を生成する手法もあります。どういったものがあるか調査してみましょう。\n",
    "\n",
    "**※省略**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
